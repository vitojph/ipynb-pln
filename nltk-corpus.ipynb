{
 "metadata": {
  "name": "nltk-corpus"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Resumen NLTK: Acceso a corpus de texto y recursos l\u00e9xicos\n",
      "\n",
      "Este resumen se corresponde con el cap\u00edtulo 2 del NLTK Book [Accessing Text Corpora and Lexical Resources](http://nltk.googlecode.com/svn/trunk/doc/book/ch02.html). La lectura del cap\u00edtulo es muy recomendable.\n",
      "\n",
      "\n",
      "## Corpus no anotados: el Proyecto Gutenberg\n",
      "\n",
      "NLTK no da acceso directo a varias colecciones de textos. Para empezar, vamos a juguetear un poco con los libros del [Proyecto Gutenberg](http://www.gutenberg.org), un repositorio p\u00fablico de libros libres y/o sin derechos de copyright en vigor.\n",
      "Antes de nada, necesitamos importar el m\u00f3dulo `gutenberg` que est\u00e1 en la librer\u00eda `nltk.corpus`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import gutenberg\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Podemos listar el cat\u00e1alogo de libros del Proyecto Gutenberg disponibles desde NLTK a trav\u00e9s del m\u00e9todo `nltk.corpus.gutenberg.fileids`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gutenberg.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para cargar alguno de estos libros en variables y poder manipularlos directamente, podemos utilizar varios m\u00e9todos.\n",
      "\n",
      "- `gutenberg.raw` recupera el texto como una \u00fanica cadena de caracteres.\n",
      "- `gutenberg.words` recupera el texto tokenizado en palabras. El m\u00e9todo devuelve una lista palabras.\n",
      "- `gutenberg.sents` recupera el texto segmentado por oraciones. El m\u00e9todo devuelve una lista de oraciones. Cada oraci\u00f3n es a su vez una lista de palabras.\n",
      "- `gutenberg.paras` recupera el texto  segmentado por p\u00e1rrafos. El m\u00e9todo devuelve una lista de p\u00e1rrafos.  Cada p\u00e1rrafo es una lista de oraciones, cada oraci\u00f3n es a su vez una lista de palabras."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "texto = gutenberg.raw(\"carroll-alice.txt\")\n",
      "print texto[:200] # imprimo los primeros 200 caracteres del libro"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "palabras = gutenberg.words(\"carroll-alice.txt\")\n",
      "print palabras[:20] # imprimo las primeros 20 palabras"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oraciones = gutenberg.sents(\"carroll-alice.txt\")\n",
      "print oraciones[-3] # imprimo la antepen\u00faltima oraci\u00f3n "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parrafos = gutenberg.paras(\"carroll-alice.txt\")\n",
      "print parrafos[-1] # imprimo el \u00faltimo p\u00e1rrafo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate en que cada m\u00e9todo devuelve una estructura de datos diferente: desde una \u00fanica cadena a listas de listas anidadas. Para que tengas claro las dimensiones de cada uno, podemos imprimir el n\u00famero de caracteres, palabras, oraciones y p\u00e1rrafos del libro. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(texto), \"caracteres\" \n",
      "print len(palabras), \"palabras\"\n",
      "print len(oraciones), \"oraciones\"\n",
      "print len(parrafos), \"p\u00e1rrafos\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a imprimir algunas estad\u00edsticas para todos los libros del Proyecto Gutenberg disponibles. Para cada libro, impriremos por pantalla el promedio de caracteres por palabra, el promedio de palabras por oraci\u00f3n y el promedio de oraciones por p\u00e1rrafo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for libro in gutenberg.fileids():\n",
      "    caracteres = len(gutenberg.raw(libro))\n",
      "    palabras = len(gutenberg.words(libro))\n",
      "    oraciones = len(gutenberg.sents(libro))\n",
      "    parrafos = len(gutenberg.paras(libro))\n",
      "    print libro[:-4], caracteres/palabras, palabras/oraciones, oraciones/parrafos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los libros del Proyecto Gutenberg constituyen el tipo de corpus m\u00e1s sencillo: no est\u00e1 anotado (no incluye ning\u00fan tipo de informaci\u00f3n ling\u00fc\u00edstica) ni categorizado. \n",
      "\n",
      "## Corpus categorizados y anotados: el Corpus de Brown\n",
      "\n",
      "El Corpus de Brown fue el primer gran corpus orientado a tareas de PLN. Desarrollado en la Universidad de Brown, contiene m\u00e1s de un mill\u00f3n de palabras provenientes de 500 fuentes.  La principal catacter\u00edstica de este corpus es que sus textos est\u00e1n categorizados por g\u00e9nero. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como en los libros del Proyecto Gutenberg, aqu\u00ed tambi\u00e9n podemos imprimir los nombres de los ficheros. En este caso son poco significativos, nos nos dicen nada del contenido."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(brown.fileids())\n",
      "print brown.fileids()[:10] # Brown est\u00e1 formado por 500 documentos, \n",
      "# imprimimos solos los 10 primeros"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sin embargo, este corpus est\u00e1 categorizado: los textos est\u00e1n agrupados seg\u00fan su tem\u00e1tica. Y en este caso, los nombres de las categor\u00edas s\u00ed nos permiten intuir el contenido de los textos."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print brown.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "De manera similar a los libros del Proyecto Gutenberg, podemos acceder a los textos de este corpus a trav\u00e9s de los m\u00e9todos `brown.raw`, `brown.words`, `brown.sents` y `brown.paras`. Adem\u00e1s, podemos acceder a una categor\u00eda de textos concretas si lo especificamos como argumento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "news_words = brown.words(categories=\"news\")\n",
      "scifi_sents = brown.sents(categories=\"science_fiction\") "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print news_words[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print scifi_sents[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a sacar provecho de la categorizaci\u00f3n de los textos de este corpus. Para ello, vamos a calcular la frecuencia de distribuci\u00f3n de distintos verbos modales para cada categor\u00eda. Para ello vamos a calcular una distribuci\u00f3n de frecuencia condicional que calcule la frecuencia de cada palabra para cada categor\u00eda.\n",
      "\n",
      "No te preocupes si no entiendes la sintaxis para calcular `cfd` a trav\u00e9s del objeto `ConditionalFreqDist`. Cr\u00e9eme, ese objeto calcula frecuencias de palabras atendiendo a la categor\u00eda en la que aparecen. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk import ConditionalFreqDist\n",
      "modals = \"can could would should must may might\".split()\n",
      "modals_cfd = ConditionalFreqDist(\n",
      "                          (category, word) \n",
      "                          for category in brown.categories() \n",
      "                          for word in brown.words(categories=category))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Una vez tenemos calculada la frecuencia de distribuci\u00f3n condicional, podemos pintar los valores f\u00e1cilmente en forma de tabla a trav\u00e9s del m\u00e9todo `.tabulate`, especificando como condiciones cada una de las categor\u00edas, y como muestras los verbos modales del ingl\u00e9s que hemos definido."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modals_cfd.tabulate(conditions=brown.categories(), samples=modals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tambi\u00e9n podemos representar la distribuci\u00f3n de los verbos modales en forma de gr\u00e1fica, utilizando el m\u00e9todo `.plot`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modals_cfd.plot(conditions=[\"news\", \"science_fiction\", \"hobbies\", \n",
      "                            \"belles_lettres\", \"learned\"], samples=modals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El corpus de Brown no solo est\u00e1 categor\u00edzado, tambi\u00e9n est\u00e1 anotado con informaci\u00f3n morfol\u00f3gica. Para acceder a la versi\u00f3n anotada del corpus, podemos utilizar los m\u00e9todos: `brown.tagged_words`, `brown.tagged_sents` y `brown.tagged_\n",
      "paras`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scifi_tagged_words = brown.tagged_words(categories=\"science_fiction\")\n",
      "print scifi_tagged_words[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que cuando accedemos a la versi\u00f3n etiquetada del corpus, no obtenemos una simple lista de palabras sino una lista de tuplas, donde el primer elemento es la palabra en cuesti\u00f3n u el segundo es la etiqueta que indica la categor\u00eda gramatical de la palabra.\n",
      "\n",
      "Este conjunto etiquetas se ha convertido en casi un est\u00e1ndar para el ingl\u00e9s y se utilizan habitualmente para anotar cualquier recurso ling\u00fc\u00edstico en esa lengua.\n",
      "\n",
      "Vamos a crear una nueva frecuencia de distribuci\u00f3n condicional para calcular la frecuencia de aparici\u00f3n de las etiquetas, teniendo en cuenta la categor\u00eda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tags_cfd = ConditionalFreqDist(\n",
      "                              (category, item[1])\n",
      "                              for category in brown.categories()\n",
      "                              for item in brown.tagged_words(categories=category)\n",
      "                              )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Y ahora vamos a imprimir la tabla de frecuencias para cada categor\u00eda y para algunas de las etiquetas morfol\u00f3gicas: sustantivos en singular `NN`, verbos en presente `VB`, verbos en pasado simple `VBD`, participios pasados `VBN`, adjetivos `JJ`, preposiciones `IN`, y art\u00edculos `AT`. Estas cifras no son directamente comparables entre categor\u00edas, ya que \u00e9stas no est\u00e1n equilibradas: hay categor\u00edas con m\u00e1s textos que otras."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags_cfd.tabulate(conditions=brown.categories(), \n",
      "                  samples=\"NN VB VBD VBN JJ IN AT\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Recursos L\u00e9xicos: WordNet\n",
      "\n",
      "Wordnet es una red sem\u00e1ntica para el ingl\u00e9s. En esencia, es similar a un diccionario pero est\u00e1 organizado por *synsets* (conjunto de palabras sin\u00f3nimas) y no por lemas. \n",
      "\n",
      "Podemos acceder a WordNet a trav\u00e9s de NLTK:\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import wordnet as wn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para consultar los synsets en los que aparece una determinada palabra, podemos utilizar el m\u00e9todo `.synsets` como se muestra en el ejemplo. Como resultado obtenemos una lista con todos los synsets en los que aparece la palabra."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print wn.synsets(\"sword\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este caso, la palabra *sword* solo aparece en un synset, lo que implica que solo tiene un sentido. Adem\u00e1s, sabemos que es un sustantivo, porque el nombre de synset est\u00e1 etiquetado como `n`.\n",
      "\n",
      "Si guardo el synset en cuesti\u00f3n en una variable (f\u00edjate que me quedo con el primer elemento de la lista que me devuelve el m\u00e9todo `wn.synsets`), podemos acceder a distintos m\u00e9todos:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sword = wn.synsets(\"sword\")[0]\n",
      "print sword.lemma_names # imprime los lemas del synset => sin\u00f3nimos\n",
      "print sword.definition # imprime la definici\u00f3n del synset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Si escribes `sword.` y pulsas el tabulador podr\u00e1s visualizar todos los m\u00e9todos accesibles desde un objeto synset. Son muchos: si tienes inter\u00e9s en alguno que no se menciona en este resumen, preg\u00fantame o consulta el libro de NLTK.\n",
      "\n",
      "Entre las cosas que s\u00ed nos interesan est\u00e1 el poder acceder a relaciones como hiponimia, meronimia, etc. Por ejemplo, para acceder a todos los hip\u00f3nimos de *sword* con el sentido de *espada*, es decir, a todos los **tipos de** *espada* y a sus definiciones."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in sword.hyponyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En lugar de bajar hasta los elementos m\u00e1s espec\u00edficos, podemos navegar en la jerarqu\u00eda de sentidos hasta los synsets m\u00e1s generales. Por ejemplo, podemos acceder a los hiper\u00f3nimos inmediatos de un synset a trav\u00e9s del m\u00e9todo `.hypernyms()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in sword.hypernyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que con este m\u00e9todo s\u00f3lo subimos un nivel hacia el synset m\u00e1s general. En este caso, comprobamos que *sword* es un tipo de *weapon* o *arm*. Si, por el contrario, lo que nos interesa es acceder a todos los hiper\u00f3nimos de *sword*, navegando hasta el elemento ra\u00edz de la jerarqu\u00eda de WordNet (que siempre es *entity*), podemos utilizar el m\u00e9todo `.hypernym_paths()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for path in sword.hypernym_paths():\n",
      "    for element in path:\n",
      "        print element.lemma_names\n",
      "        print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate que `.hypernym_paths()` me devuelve una lista de caminos posibles desde el synset en cuesti\u00f3n hasta el elemento *entity*. Por eso itero sobre los elementos `path` que me devuelve `.hypernym_paths()`. En el ejemplo de *sword*, da la casualidad de que solo hay un camino posible. Cada `path` es una lista de synsets, e itero sobre ellos. Por eso utilizo un bucle dentro de otro.\n",
      "\n",
      "Para acceder a los mer\u00f3nimos, es decir, a las partes o elementos constitutivos de *sword*, podemos utilizar el m\u00e9todo `.part_meronyms()`, como se muestra a continuaci\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for element in sword.part_meronyms():\n",
      "    print element.lemma_names\n",
      "    print element.definition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "De manera similar, podemos acceder a los hol\u00f3nimos de un synset, es decir, a los elementos de los que *espada* forma parte, a trav\u00e9s del m\u00e9todo `.part_holonyms()`. El synset que estamos utilizando no tiene definidos hol\u00f3nimos, as\u00ed que el ejemplo devuelve una lista vac\u00eda::"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sword.part_holonyms()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hay varios m\u00e9todos para acceder a distintos tipos de mer\u00f3nimos y hol\u00f3nimos, aunque no siempre est\u00e1n definidas estas relaciones. Cuando no est\u00e1n definidas, los m\u00e9todos no dan error, simplemente devuelven listas vac\u00edas.\n",
      "\n",
      "Los nombres de estos m\u00e9todos tratan de ser autoexplicativos: por un lado, tenemos `.part_holonyms()`, `.member_holonyms()`, `.substance_holonyms()`, y por otro, `.part_meronyms()`, `.member_meronyms()`, `.substance_meronyms()`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}