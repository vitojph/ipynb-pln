{
 "metadata": {
  "name": "nltk-analyzers"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Resumen de NLTK: An\u00e1lisis sint\u00e1ctico\n",
      "\n",
      "Este resumen se corresponde con el cap\u00edtulo 8 del NLTK Book [Analyzing Sentence Structure](http://nltk.googlecode.com/svn/trunk/doc/book/ch08.html). La lectura del cap\u00edtulo es muy recomendable.\n",
      "\n",
      "\n",
      "En este resumen vamos a repasar c\u00f3mo crear gram\u00e1ticas con NLTK y c\u00f3mo crear herramientas que nos permitan analizar sint\u00e1cticamente oraciones sencillas.\n",
      "\n",
      "Para empezar, necesitamos importar el m\u00f3dulo `nltk` que nos da acceso a todas las funcionalidades:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gram\u00e1ticas Independientes del Contexto (CFG)\n",
      "\n",
      "Noam Chmosky defini\u00f3 una [jerarqu\u00eda de lenguajes y gram\u00e1ticas](http://es.wikipedia.org/wiki/Jerarqu%C3%ADa_de_Chomsky) que se utiliza habitualmene en Ling\u00fc\u00edstica e Inform\u00e1tica para clasificar lenguajes y gram\u00e1ticas formales. Cuando queremos modelar fen\u00f3menos ling\u00fc\u00edsticos de las lenguas naturales, el tipo de gram\u00e1tica m\u00e1s adeacuado es el conocido como Tipo 2 o [Gram\u00e1ticas Independientes del Contexto](http://es.wikipedia.org/wiki/Gram%C3%A1tica_libre_de_contexto) o *Context-Free Grammars (CFG)* en ingl\u00e9s.\n",
      "\n",
      "Vamos a definir una gram\u00e1tica simplemente como un conjunto de reglas de reescritura o transformaci\u00f3n. Sin entrar en muchos detalles sobre las restricciones que tienen que cumplir las reglas de las gram\u00e1ticas de Tipo 2, es importante que tengamos en cuenta lo siguiente:\n",
      "\n",
      "- Las gram\u00e1ticas formales manejan dos tipos de alfabetos.\n",
      "  1. Los **s\u00edmbolos no terminales** son los componentes intermedios que utilizamos en las reglas. Todo s\u00edmbolo no terminal tiene que ser definido como una secuenca de otros s\u00edmbolos. En nuestro caso, los no terminales van a ser las categor\u00edas sint\u00e1cticas. \n",
      "  2. Los **s\u00edmbolos terminales** son los componentes finales reconocidos por la gram\u00e1tica. En nuestro caso, los terminales van a ser las palabras de las oraciones que queremos analizar sint\u00e1cticamente.\n",
      "- Todas las reglas de una gram\u00e1tica formal tienen la forma `S\u00edmbolo1 -> S\u00edmbolo2, S\u00edmbolo3... S\u00edmboloN` y se leen como *el `S\u00edmbolo1` se define/est\u00e1 formado/se reescribe como una secuencia formada por `S\u00edmbolo2`, `S\u00edmbolo3`, etc.*\n",
      "- En las gram\u00e1ticas independientes del contexto, la parte situada a la derecha de la flecha `->` es siempre un \u00fanico s\u00edmbolo no terminal.\n",
      "\n",
      "## Gram\u00e1ticas Generativas en NLTK\n",
      "\n",
      "Pues bien, para definir nuestras gram\u00e1ticas en NLTK podemos escribirlas en un fichero aparte o como una cadena de texto siguiendo el formalismo de las gramaticas generativas de Chomsky. Vamos a definir una sencilla gram\u00e1tica capaz de reconocer la famosa frase de los hermanos Marx *I shot an elephant in my pajamas*, y la vamos a guardar como una cadena de texto en la variable `g1`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "g1 = \"\"\"\n",
      "S -> NP VP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "PP -> P NP\n",
      "Det -> 'an' | 'my'\n",
      "N -> 'elephant' | 'pajamas'\n",
      "V -> 'shot'\n",
      "P -> 'in'\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00edjate c\u00f3mo hemos definido nuestra gram\u00e1tica:\n",
      "\n",
      "- Hemos encerrado todo entre triples comillas dobles. Recuerda que esta sintaxis de Python permite crear cadenas que contengan retornos de carro y ocupen m\u00e1s de una l\u00ednea de longitud.\n",
      "- Para los no terminales utilizamos las convenciones habituales para las estructuras sint\u00e1cticas y las categor\u00edas de palabras y los escribimos en may\u00fasculas. Las etiquetas son autoexplicativas, aunque est\u00e9n en ingl\u00e9s.\n",
      "- Lo no terminales van escritos entre comillas simples.\n",
      "- Cuando un no terminal se puede definir de m\u00e1s de una forma, marcamos la disyunci\u00f3n con la barra vertical `|`. \n",
      "- Tenemos reglas que se interpretan de la siguiente manera: *una oraci\u00f3n se define como una sintagma nominal y un sintagma verbal*; *un sintagma nominal se define como un determinante y un nombre, o un determinante, un nombre y un sintagma preposicional, o la palabra I*, etc.\n",
      "\n",
      "A partir de nuestra gram\u00e1tica en una cadena de texto, necesitamos crear un objeto `ContextFreeGrammar` que podamos utilizar posterioremente. Para ello, es imprescindible parsearla antes con el m\u00e9todo `nltk.parse_cfg()`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "grammar1 = nltk.parse_cfg(g1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Con el objeto `grammar1` ya creado, creamos un analizador con el m\u00e9todo `nltk.ChatParser`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "analyzer = nltk.ChartParser(grammar1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Una vez creado nuestro analizador ya lo podemos utilizar. Tenemos a nuestro alcance el m\u00e9todo `.parse` para analizar sint\u00e1cticamente cualquier oraci\u00f3n que se especifique como una cadena de palabras. Nuestra gram\u00e1tica es bastante limitada, pero podemos utilizarla para analizar la oraci\u00f3n *I shot an elephant in my pajamas*. Si imprimimos el resultado del m\u00e9todo, obtenemos el \u00e1rbol sint\u00e1ctico."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oracion = \"I shot an elephant in my pajamas\".split()\n",
      "print analyzer.parse(oracion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP I)\n",
        "  (VP\n",
        "    (V shot)\n",
        "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Por si no te has dado cuenta, la oraci\u00f3n *I shot an elephant in my pajamas* es ambigua en ingl\u00e9s. Existe una doble interpretaci\u00f3n para el sintagma preposicional *in my pajamas*: En el momento del disparo, \u00bfqui\u00e9n llevaba puesto el pijama? \u00bfEl elefante o yo? \n",
      "\n",
      "Pues bien, nuestra gram\u00e1tica recoge esta ambig\u00fcedad y ser\u00eda capaz de analizarla de dos maneras diferentes. Para acceder, cuando sea el caso, a todos los posibles \u00e1rboles sint\u00e1cticos podemos ejecutar el m\u00e9todo `\".nbest_parse()`. Dicho m\u00e9todo devuelve una lista de \u00e1rboles sint\u00e1cticos. Si queremos imprimir todos por pantalla, tenemos que crear un bucle e iterar sobre el resultado del m\u00e9todo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for arbol in analyzer.nbest_parse(oracion):\n",
      "    print arbol"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP I)\n",
        "  (VP\n",
        "    (V shot)\n",
        "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
        "(S\n",
        "  (NP I)\n",
        "  (VP\n",
        "    (VP (V shot) (NP (Det an) (N elephant)))\n",
        "    (PP (P in) (NP (Det my) (N pajamas)))))\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recuerda que para imprimir el \u00e1rbol sint\u00e1ctico hay que usar expl\u00edcitamente `print`. Si no lo haces, los m\u00e9todos devuelven una estructura de datos un poco compleja y dif\u00edcil de leer. Compara el siguiente resultado con el anterior."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer.parse(oracion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "Tree('S', [Tree('NP', ['I']), Tree('VP', [Tree('V', ['shot']), Tree('NP', [Tree('Det', ['an']), Tree('N', ['elephant']), Tree('PP', [Tree('P', ['in']), Tree('NP', [Tree('Det', ['my']), Tree('N', ['pajamas'])])])])])])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### NOTA IMPORTANTE\n",
      "Cuando un analizador reconoce todo el vocabulario de una oraci\u00f3n de entrada pero es incapaz de analizarla, el m\u00e9todo `parse()` no da error pero devuelve un objeto `None`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyzer.parse(\"shot an pajamas elephant my I\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sin embargo, cuando el analizador no reconoce todo el vocabulario (porque utilizemos una palabra no definida dentro del l\u00e9xico), el m\u00e9todo `parse()` falla y muestra un mensaje como el siguiente. F\u00edjate solo en la \u00faltima l\u00ednea:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analyzer.parse(\"our time is running out\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Grammar does not cover some of the input words: \"'our', 'time', 'is', 'running', 'out'\".",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-e185af90321b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"our time is running out\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/nltk/parse/api.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, sent)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbest_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/nltk/parse/chart.pyc\u001b[0m in \u001b[0;36mnbest_parse\u001b[1;34m(self, tokens, n, tree_class)\u001b[0m\n\u001b[0;32m   1403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnbest_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1406\u001b[0m         \u001b[1;31m# Return a list of complete parses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/nltk/parse/chart.pyc\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/nltk/grammar.pyc\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[1;32m--> 608\u001b[1;33m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;31m# [xx] does this still get used anywhere, or does check_coverage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'our', 'time', 'is', 'running', 'out'\"."
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visto un primer ejemplo de CFG, vamos a cambiar de lengua y crear un analizador para oraciones sencillas en espa\u00f1ol. El procedimiento es el mismo, definimos nuestra gram\u00e1tica en formato de Chomsky en un fichero aparte o en una cadena de texto, la parseamos con el m\u00e9todo `nltk.parse_cfg()` y creamos un analizador con el m\u00e9todo `nltk.ChartParser()`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "g2 = \"\"\"\n",
      "O -> SN SV\n",
      "SN -> Det N | Det N Adj | Det Adj N | NProp\n",
      "SV -> V | V SN | v SP| V SN SP\n",
      "SP -> Prep SN\n",
      "Det -> 'el' | 'la' | 'un' | 'una' \n",
      "N -> 'ni\u00f1o' | 'ni\u00f1a' | 'manzana' | 'pera' | 'cuchillo'\n",
      "NProp -> 'Juan' | 'Ana' | 'Perico' \n",
      "Adj -> 'bonito' | 'peque\u00f1a' | 'verde'\n",
      "V -> 'come' | 'salta' | 'pela' | 'persigue_a'\n",
      "Prep -> 'de' | 'con' | 'desde'\n",
      "\"\"\"\n",
      "\n",
      "grammar2 = nltk.parse_cfg(g2)\n",
      "analizador2 = nltk.ChartParser(grammar2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a probar si es capaz de analizar distintas oraciones es espa\u00f1ol. Para hacerlo m\u00e1s divertido, vamos a guardar varias oraciones separadas por un intro (simbolizado por el metacaracter `\\n`) en una lista de cadenas llamda `oraciones`. Iteramos sobre esas oraciones, las imprimimos, despu\u00e9s las rompemos en listas de palabras (con el m\u00e9todo `.split()`) e imprimimos el resultado de analizarlas con nuestro analizador.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oraciones = \"\"\"Ana salta\n",
      "la ni\u00f1a pela una manzana verde con el cuchillo\n",
      "Juan come un cuchillo bonito desde el ni\u00f1o\n",
      "un manzana bonito salta el cuchillo desde el ni\u00f1o verde\n",
      "el cuchillo verde persigue_a la peque\u00f1a manzana de Ana\"\"\".split(\"\\n\")\n",
      "\n",
      "for oracion in oraciones:\n",
      "    print oracion\n",
      "    print analizador2.parse(oracion.split())    \n",
      "    print \"\\n\" # imprimimos un intro de m\u00e1s, para separar el resultado"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ana salta\n",
        "(O (SN (NProp Ana)) (SV (V salta)))\n",
        "\n",
        "\n",
        "la ni\u00f1a pela una manzana verde con el cuchillo\n",
        "(O\n",
        "  (SN (Det la) (N ni\u00f1a))\n",
        "  (SV\n",
        "    (V pela)\n",
        "    (SN (Det una) (N manzana) (Adj verde))\n",
        "    (SP (Prep con) (SN (Det el) (N cuchillo)))))\n",
        "\n",
        "\n",
        "Juan come un cuchillo bonito desde el ni\u00f1o\n",
        "(O\n",
        "  (SN (NProp Juan))\n",
        "  (SV\n",
        "    (V come)\n",
        "    (SN (Det un) (N cuchillo) (Adj bonito))\n",
        "    (SP (Prep desde) (SN (Det el) (N ni\u00f1o)))))\n",
        "\n",
        "\n",
        "un manzana bonito salta el cuchillo desde el ni\u00f1o verde\n",
        "(O\n",
        "  (SN (Det un) (N manzana) (Adj bonito))\n",
        "  (SV\n",
        "    (V salta)\n",
        "    (SN (Det el) (N cuchillo))\n",
        "    (SP (Prep desde) (SN (Det el) (N ni\u00f1o) (Adj verde)))))\n",
        "\n",
        "\n",
        "el cuchillo verde persigue_a la peque\u00f1a manzana de Ana\n",
        "(O\n",
        "  (SN (Det el) (N cuchillo) (Adj verde))\n",
        "  (SV\n",
        "    (V persigue_a)\n",
        "    (SN (Det la) (Adj peque\u00f1a) (N manzana))\n",
        "    (SP (Prep de) (SN (NProp Ana)))))\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vamos a aumentar la cobertura de nuestra gram\u00e1tica de modo que sea capaz de reconocer y analizar oraciones coordinadas. Para ello, modificamos la regla en la que definimos la oraci\u00f3n a\u00f1adiendo una definici\u00f3n recursivaque defina oraci\u00f3n como la secuencia de una oraci\u00f3n (`O`) seguida de una conjunci\u00f3n (`Conj`) y de otra oraci\u00f3n (`O`). Por \u00faltimo a\u00f1adimos tambi\u00e9n algo de l\u00e9xico nuevo: un par de conjunciones."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "g3 = \"\"\"\n",
      "O -> SN SV | O Conj O\n",
      "SN -> Det N | Det N Adj | Det Adj N | NProp\n",
      "SV -> V | V SN | v SP| V SN SP\n",
      "SP -> Prep SN\n",
      "Det -> 'el' | 'la' | 'un' | 'una' \n",
      "N -> 'ni\u00f1o' | 'ni\u00f1a' | 'manzana' | 'pera' | 'cuchillo'\n",
      "NProp -> 'Juan' | 'Ana' | 'Perico' \n",
      "Adj -> 'bonito' | 'peque\u00f1a' | 'verde'\n",
      "V -> 'come' | 'salta' | 'pela' | 'persigue_a'\n",
      "Prep -> 'de' | 'con' | 'desde'\n",
      "Conj -> 'y' | 'pero'\n",
      "\"\"\"\n",
      "\n",
      "# Ahora fijate c\u00f3mo creamos en analizador en un solo paso\n",
      "# comp\u00e1ralo con los ejemplos anteriores\n",
      "analizador3 = nltk.ChartParser(nltk.parse_cfg(g3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print analizador3.parse(\"\"\"la manzana salta y el ni\u00f1o come\n",
      "pero el cuchillo verde persigue_a la peque\u00f1a manzana de Ana\"\"\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(O\n",
        "  (O\n",
        "    (O (SN (Det la) (N manzana)) (SV (V salta)))\n",
        "    (Conj y)\n",
        "    (O (SN (Det el) (N ni\u00f1o)) (SV (V come))))\n",
        "  (Conj pero)\n",
        "  (O\n",
        "    (SN (Det el) (N cuchillo) (Adj verde))\n",
        "    (SV\n",
        "      (V persigue_a)\n",
        "      (SN (Det la) (Adj peque\u00f1a) (N manzana))\n",
        "      (SP (Prep de) (SN (NProp Ana))))))\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recuerda que una gram\u00e1tica no es un programa: es una simple descripci\u00f3n que permite establecer qu\u00e9 estructuras sint\u00e1cticas est\u00e1n bien formadas (la oraciones gramaticales) y cu\u00e1les no (las oraciones agramaticales). Cuando una oraci\u00f3n es reconocida por una gram\u00e1tica (y en consecuencia, est\u00e1 bien formada), el analizador puede representar la estructura en forma de \u00e1rbol.\n",
      "\n",
      "NLTK proporciona acceso a distintos tipo de analizadores (\u00e1rboles de dependencias, gram\u00e1ticas probabil\u00edsticas, etc), aunque nosotros solo hemos utilizado el m\u00e1s sencillo de ellos: `nltk.ChartParser()`. Estos analizadores s\u00ed son programitas que permiten leer una gram\u00e1tica y analizar las oraciones que proporcionemos como entrada del m\u00e9todo `parse()`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}